---
title: "Homework 2"
subtitle: "BIOS 684"
author: "Jeff Oliver"
date: "15 September 2017"
output: 
  pdf_document:
  latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!--
DUE: Wednesday 20 September 2017

Instructions:
1. For each question requiring data analysis, support your conclusions by including the
relevant SAS output in your answer.
2. Include your SAS program as an Appendix to your solutions (PLEASE don’t include
the output from your program!).

Joint Modeling of Mean and Covariance in the Dental Growth Study
In a study of dental growth, measurements of the distance (mm) from the center of the
pituitary gland to the pteryomaxillary fissure were obtained on 11 girls and 16 boys at
ages 8, 10, 12 and 14 years (Potthoff and Roy, 1964).

The data are in the file “dental.txt” on the D2L dataset folder. Each row of the data
set contains the following six variables: subject ID, gender (“F” or “M”), and the
measurements at ages 8, 10, 12 and 14 years, respectively.

For all analyses in this homework, subset the dataset to contain only the
measurements for the girls.
-->

```{r echo = FALSE}
# Read in data
dental.data <- read.csv(file = "data/dental.csv",
                        header = FALSE)

# Sampling vector for later use
years <- c(8, 10, 12, 14)
years.chr <- c("08", "10", "12", "14")

# Add column names
colnames(dental.data) <- c("ID", "gender", paste0("y.", years.chr))

# Subset data
dental.data <- dental.data[dental.data$gender == "F", ]
```


## PART A: Descriptive Analyses
### 1. Plot the observed trajectories of distance for the 11 girls (all on one plot). Briefly, describe the important patterns that are apparent in the plot. (hint: use proc gplot)

The plot below shows pairwise comparisons between measurements of the same subject at different time points (below diagonal) and the correlation coefficient (r^2^) for the relationship (above diagonal):
```{r echo = FALSE}
# Function to print correlation coefficient; copied shamelessly from pairs documentation
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

# Create scatterplot of matrices to inspect data
pairs(dental.data[, 3:6], 
      labels = c("Year 8", "Year 10", "Year 12", "Year 14"),
      upper.panel = panel.cor)
```

There is high **within-subject positive correlation** between pairs of sampling times. Interestingly, the within-subject correlation does not appear to decrease much with time (e.g. the correlation coefficient between year 8 and year 10 is effectively the same as between year 8 and year 14).

### 2. Obtain descriptive statistics for the distances in girls including means and standard deviations at each measurement occasion. Plot the means against time and comment on the shape of their trajectory. Also, comment on how the standard deviations are changing with time.

```{r echo = FALSE}
distance.means <- colMeans(x = dental.data[, 3:6])
distance.sd <- apply(X = dental.data[, 3:6], MARGIN = 2, FUN = sd)
```

|Year|Mean|Standard Deviation|
|:--:|:--:|:----------------:|
| `r years[1]` | `r round(distance.means[1], 2)` | `r round(distance.sd[1], 2)` |
| `r years[2]` | `r round(distance.means[2], 2)` | `r round(distance.sd[2], 2)` |
| `r years[3]` | `r round(distance.means[3], 2)` | `r round(distance.sd[3], 2)` |
| `r years[4]` | `r round(distance.means[4], 2)` | `r round(distance.sd[4], 2)` |

```{r echo = FALSE}
plot(x = c(8, 10, 12, 14),
     y = distance.means,
     xlab = "Measurement year",
     ylab = "Mean distance (mm)",
     main = "Mean distance over time")
```

The mean distance between the center of the pituitary gland to the pteryomaxillary fissure is increasing over time.

```{r echo = FALSE}
plot(x = c(8, 10, 12, 14),
     y = distance.sd,
     xlab = "Measurement year",
     ylab = "Distance standard deviation (mm)",
     main = "Standard deviation over time")
```

Standard deviations are _not_ constant through time, illustrating **heterogeneity in variation**. In general, standard deviations appear to increase over time, although this is not absolute, as year 10 has the lowest standard deviation.

### 3. Obtain the variance-covariance and correlation matrices for the repeated measurements over time. Briefly, describe the important patterns in the correlations.

The variance-covariance matrix:
```{r echo = FALSE}
dental.var.cov <- cov(dental.data[, 3:6])
dental.var.cov
```

The correlation matrix:
```{r echo = FALSE}
cor(dental.data[, 3:6])
```

There is high within-subject correlation between measurements at different times ($|\rho| >> 0$ for all pairwise comparisons).

### 4. Identify one unexpected feature of this correlation structure generated in question 3. Provide a possible reason for this unexpected feature.

There appears little change in the correlation matrix over time. This is in contrast to the expectation that correlations should decrease with increasing time separation. There could be little room for change to occur over the developmental period in which the measurements were taken. That is, if most of the growth between the pituitary gland to the pteryomaxillary fissure occurs before age eight in girls, we would expect little change in the correlation between distance measurements at different time points after eight years.

## PART B: Repeated Measures Models
### 5. Use PROC MIXED to fit a repeated measures model in which there is separate parameter for the mean distance at each of the four ages [HINT: Use the NOINT option on the MODEL statement]. Use an unstructured variance-covariance structure. For estimating parameters, fit the model using (i) the METHOD=ML option and (ii) the METHOD=REML option on the PROC MIXED statement.

```{r echo = FALSE}
library("tidyr")
# Convert to "long format"
dental.long <- gather(data = dental.data, 
                         key = year, 
                         value = distance,
                         -ID, -gender)
# Add column for treating year as a number
dental.long$year.num <- as.numeric(gsub(pattern = "y.", "", x = dental.long$year))

# Make sure our year column is treated as a factor (and set year 8 as reference)
dental.long$year <- factor(dental.long$year, levels = paste0("y.", years.chr))

# Make sure our ID column is treated as a factor
dental.long$ID <- factor(dental.long$ID)

library("nlme")
# Run full model with parameter for each of the four ages using *ML* parameter estimation
full.ML <- gls(distance ~ year,
                data = dental.long,
                correlation = corSymm(form = ~1|ID), # Unstructured variance-covariance
                weights = varIdent(form = ~ 1|year), 
                method = "ML")
# Get variance-covariance matrix for ML estimate:
full.ML.cov <- getVarCov(obj = full.ML)
rownames(full.ML.cov) <- colnames(full.ML.cov) <- paste0("y.", years.chr)

# Run full model with parameter for each of the four ages using *REML* parameter estimation
full.REML <- gls(distance ~ year,
                 data = dental.long,
                 correlation = corSymm(form = ~1|ID), # Unstructured variance-covariance
                 weights = varIdent(form = ~ 1|year), 
                 method = "REML")
# Get variance-covariance matrix for ML estimate:
full.REML.cov <- getVarCov(obj = full.REML)
rownames(full.REML.cov) <- colnames(full.REML.cov) <- paste0("y.", years.chr)
```

#### a. Does either method of estimation give the same estimates of the variances and covariances as in your answer to question 3? If so, which one or ones?

The ML variance-covariance matrix:
```{r echo = FALSE}
full.ML.cov[]
```

The REML variance-covariance matrix:
```{r echo = FALSE}
full.REML.cov[]
```

Neither estimation approach produced a variance-covariance matrix identical to the one from question 3, although the REML estimation approach did come a little closer than the ML estimation.

The difference between the ML-estimate variance-covariance matrix and the matrix for question 3:
```{r echo = FALSE}
round(full.ML.cov[] - dental.var.cov, 2)
```

The difference between the REML-estimate variance-covariance matrix and the matrix for question 3:
```{r echo = FALSE}
round(full.REML.cov[] - dental.var.cov, 2)
```

#### b. Compare the variances and covariances for the two methods of estimation (i.e. ML versus REML). If they are not the same, comment on why they are different and state which method should be preferred and why?

The variance-covariance estimates from the REML-estimation are all higher than the estimates from the ML-estimation; note the matrix of these differences (REML - ML) is comprised of all positive values:

```{r echo = FALSE}
round(full.REML.cov[] - full.ML.cov[], 2)
```

This is expected because while the ML and REML estimators for $\sigma^2$ have identical numerators, the ML-estimate has the denominator $K$ (the product of the number of subjects and the number of sampling points, $N \times n$) while the REML-estimate has the denominator $K - p$, where $p$ corresponds to the number of $\beta$ coefficients. The ML estimator of $\sigma^2$ is biased because it is based on maximizing a likelihood function which includes $\beta$ coefficients which themselves are estimates. The REML estimator is unbiased because it corrects for the fact that the $\beta$s are estimates by subtracting an additional term in the likelihood function, based on the residuals.  
If we are interested in unbiased sample of the variance-covariance matrix, the REML approach would be the most appropriate one to use.

#### c. Compare the estimated betas in the regression model to the sample means calculated in question 2. Provide a reason for any relationship you notice between these two sets of estimates.

Linear combinations of the estimated $\beta$ coefficients in both the ML and REML are equal to the sample means for each time sampling point. This is due to the design structure of our regression models:

$$
X_{i}
=
\begin{pmatrix}
1 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1 & 0 & 1 & 0 \\
1 & 0 & 0 & 1 \\
\end{pmatrix}
$$

Where $X_{ij}$ is equal to 1 if the response was observed at time point $j$ and 0 otherwise. This leads to the following linear combinations

$$
\begin{aligned}
\begin{array}{l}
\mu_{1} = E(Y_{i1}) = \beta_{1} \\
\mu_{2} = E(Y_{i2}) = \beta_{1} + \beta_{2} \\
\mu_{3} = E(Y_{i3}) = \beta_{1} + \beta_{3} \\
\mu_{4} = E(Y_{i4}) = \beta_{1} + \beta_{4} \\
\end{array} 
\end{aligned}
$$
Using the coefficient estimates from the REML model:

$\mu_{1} =$ `r as.character(round(full.REML$coefficients[[1]], 2))`  
$\mu_{2} =$ `r as.character(round(full.REML$coefficients[[1]], 2))` + `r as.character(round(full.REML$coefficients[[2]], 2))` = `r as.character(round(full.REML$coefficients[[1]] + full.REML$coefficients[[2]], 2))`  
$\mu_{3} =$ `r as.character(round(full.REML$coefficients[[1]], 2))` + `r as.character(round(full.REML$coefficients[[3]], 2))` = `r as.character(round(full.REML$coefficients[[1]] + full.REML$coefficients[[3]], 2))`  
$\mu_{4} =$ `r as.character(round(full.REML$coefficients[[1]], 2))` + `r as.character(round(full.REML$coefficients[[4]], 2))` = `r as.character(round(full.REML$coefficients[[1]] + full.REML$coefficients[[4]], 2))`  
these correspond to the sample means presented in the table for question 2.
  
Note: the sample R code had a slightly different model specification in the call to `gls`: `distance ~ time - 1` (I used `distance ~ time`). While the former resulted in $\beta$ coefficients that were identical to the sample means, I did not quite understand how the `-1` caused this, so I did not do it that way.

### 6. Using METHOD=ML and an unstructured variance-covariance structure, fit a model which assumes that the mean distance changes linearly with time.

```{r echo = FALSE}
# Run linear model using *ML* parameter estimation
linear.ML <- gls(distance ~ year.num,
                 data = dental.long,
                 correlation = corSymm(form = ~1|ID), # Unstructured variance-covariance
                 weights = varIdent(form = ~ 1|year.num), 
                 method = "ML")
```

#### a. Why can this model be considered to be nested within the model fitted in question 5?
This model is nested within the full ML model because there is a linear combination of the full model parameters that effectively reduces down to the linear model, making the linear model a special case of the full model. I _think_ the following illustrates this, but I may have gotten my math wrong in some parts:

The full model specification, where each sampling point is exected to have a different mean response is:

$$
E(Y_{ij}) = \beta_{1}^F + \beta_{2}^FX_{i2} + \beta_{3}^FX_{i3} + \beta_{4}^FX_{i4}
$$
where $X_{ij} = 1$ if sampled during time point $j$, and zero otherwise; the superscript $F$ is used to distinguish these $\beta$s from those of the linear model.  

The linear model is:
$$
E(Y_{ij}) = \beta_{1} + \beta_{2}t_{ij}
$$
where $t_{ij}$ is the time the sample was taken (8, 10, 12, or 14 years).

Considering the expected values for $Y_{i}$ for each of the time points ($j = 8, 10, 12, 14$) while substituting in values for $X_{ij}$ and $t_{ij}$:

$$
\begin{aligned}
\begin{array}{l}
E(Y_{i8}) = \beta_{1}^F = \beta_{1} + 8\beta_{2} \\
E(Y_{i10}) = \beta_{1}^F + \beta_{2}^F = \beta_{1} + 10\beta_{2} \\
E(Y_{i12}) = \beta_{1}^F + \beta_{3}^F  = \beta_{1} + 12\beta_{2} \\
E(Y_{i14}) = \beta_{1}^F + \beta_{4}^F  = \beta_{1} + 14\beta_{2} \\
\end{array} 
\end{aligned}
$$

We can then solve for the values of our $\beta^F$s in terms of the $\beta$ coefficients from the linear model:

$$
\begin{aligned}
\begin{array}{l}
\beta_{1}^F = \beta_{1} + 8\beta_{2} \\
\beta_{2}^F = \beta_{1} + 10\beta_{2} - \beta_{1}^F = \beta_{1} + 10\beta_{2} - (\beta_{1} + 8\beta_{2}) = 2\beta_{2}\\
\beta_{3}^F  = \beta_{1} + 12\beta_{2} - \beta_{1}^F = \beta_{1} + 12\beta_{2} - (\beta_{1} + 8\beta_{2}) = 4\beta_{2}\\
\beta_{4}^F  = \beta_{1} + 14\beta_{2} - \beta_{1}^F = \beta_{1} + 14\beta_{2} - (\beta_{1} + 8\beta_{2}) = 6\beta_{2}\\
\end{array} 
\end{aligned}
$$
Thus the **full model can be considered a special case of the linear model**, and the latter is nested within the former, making a likelihood ratio test appropriate for model comparison.

#### b. By undertaking a likelihood ratio test, evaluate the goodness of fit of this model compared with the model fitted (with METHOD=ML) in question 5. Which model do you prefer and why?

```{r echo = FALSE}
model.compare <- anova(linear.ML, full.ML)
```

The **full model does not provide a significantly better fit than does the linear model** ($2\Delta ln(L) =$ `r round(model.compare$L.Ratio[2], 3)`, $p =$ `r round(model.compare$"p-value"[2], 3)`). I would thus prefer the linear model because it requires fewer parameters to explain the response over time.

#### c. Why should you not use the REML log likelihood to compare models in this question?

In accounting for the fact that the $\beta$ coefficients are estimated, the log-likelihood function that REML maximizes is dependent on those coefficients. Thus it is not appropriate to compare the likelihoods of mean responses between different REML models because they are based on different transformations of the responses. I am not sure if this characterization means the REML models cannot be considered nested (Fitzmaurice et al. still call them "nested"), but the difference in likelihood functions is what ultimately makes them incomparable.