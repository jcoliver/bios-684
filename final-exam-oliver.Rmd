---
title: "Final Exam"
subtitle: "BIOS 684"
author: "Jeff Oliver"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
  latex_engine: xelatex
---

```{r setup, include=FALSE, fig.height = 4}
knitr::opts_chunk$set(echo = FALSE, dev = "pdf")
library("dplyr")
library("tidyr")
library("ggplot2")
# library("lmtest")
library("gee")
# library("geepack")
# library("contrast")
library("aods3")
library("lme4")
# install.packages("alr", repos="http://R-Forge.R-project.org")
library("alr")
library("nlme")
```

## Question 1
<!--
The question concerns the analysis of a subset of data from a clinical trial which randomized patients with arthritis to one of two treatments: auranofin (A) or placebo (P). The data provided are from a subset of 51 patients. The outcome measure of interest is a binary self-assessment of arthritis status (0=poor, 1=good). The self-assessments were scheduled at weeks -1 and 0, and then at weeks 4, 8 and 12. Randomization occurred immediately after the week 0 assessment. The primary objective of the analysis is to compare the effects of auranofin and placebo on self-assessed arthritis status. 
The data from this study are stored in the file arthritis.txt, which can be found on the course web site. Each row of the data set contains the following variables: subject ID number, sex (M=male, F=female), subject’s age in years at entry to the study, the randomized treatment group (A or P), and the self-assessment results at weeks -1, 0, 4, 8 and 12, respectively. Although the subjects’ ages are provided, these data are not used in any question.
-->

```{r read-arthritis-data}
# Read data from file
arthritis.data <- read.delim(file = "data/arthritis.txt", 
                             header = FALSE,
                             sep = " ", 
                             na.strings = ".")
# Assign column names
colnames(arthritis.data) <- c("id", "sex", "age", "treatment", "r.n1", "r.0", "r.4", "r.8", "r.12")

# Re-level treatment so placebo is default
arthritis.data$treatment <- factor(arthritis.data$treatment, levels = c("P", "A"))

# Tranform to long format
arthritis.long <- gather(data = arthritis.data, 
                         key = "week.cat", 
                         value = "status",
                         -id, -sex, -age, -treatment)

# Add numeric week column
arthritis.long$week <- NA
arthritis.long$week[arthritis.long$week.cat == "r.n1"] <- -1
arthritis.long$week[arthritis.long$week.cat == "r.0"] <- 0
arthritis.long$week[arthritis.long$week.cat == "r.4"] <- 4
arthritis.long$week[arthritis.long$week.cat == "r.8"] <- 8
arthritis.long$week[arthritis.long$week.cat == "r.12"] <- 12
```

### 1. Briefly summarize the outcome data from the study in a single table. Specifically, show for each treatment group at each scheduled assessment: how many provided an assessment and what number and percentage rated their arthritis status as “good”. 
```{r summary-arthritis-data}
arthritis.table <- arthritis.long %>%
  group_by(treatment, week) %>%
  summarise(count = sum(!is.na(status)),
            num.good = sum(status == 1, na.rm = TRUE))
arthritis.table$perc.good <- 100 * (round(arthritis.table$num.good / arthritis.table$count, 4))
```

| Treatment | Week | # Assessments | # "good" | % "good"^1^ |
|:----------|:-----|:-------------:|:--------:|:----------:|
| Auranofin | -1   | `r arthritis.table$count[1]` | `r arthritis.table$num.good[1]` | `r arthritis.table$perc.good[1]` |
|           | 0    | `r arthritis.table$count[2]` | `r arthritis.table$num.good[2]` | `r arthritis.table$perc.good[2]` |
|           | 4    | `r arthritis.table$count[3]` | `r arthritis.table$num.good[3]` | `r arthritis.table$perc.good[3]` |
|           | 8    | `r arthritis.table$count[4]` | `r arthritis.table$num.good[4]` | `r arthritis.table$perc.good[4]` |
|           | 12   | `r arthritis.table$count[5]` | `r arthritis.table$num.good[5]` | `r arthritis.table$perc.good[5]` |
| Placebo   | -1   | `r arthritis.table$count[6]` | `r arthritis.table$num.good[6]` | `r arthritis.table$perc.good[6]` |
|           | 0    | `r arthritis.table$count[7]` | `r arthritis.table$num.good[7]` | `r arthritis.table$perc.good[7]` |
|           | 4    | `r arthritis.table$count[8]` | `r arthritis.table$num.good[8]` | `r arthritis.table$perc.good[8]` |
|           | 8    | `r arthritis.table$count[9]` | `r arthritis.table$num.good[9]` | `r arthritis.table$perc.good[9]` |
|           | 12   | `r arthritis.table$count[10]` | `r arthritis.table$num.good[10]` | `r arthritis.table$perc.good[10]` |
^1^Note the percent of "good" responses is based on number of assessments returned, _not_ the total number of subjects for that treatment / week combination.

#### Based on the descriptive statistics in the table, briefly characterize the pattern of change in the proportions over time including a comparison of the two treatments.

```{r plot-arthritis-status, fig.height = 3}
status.plot <- ggplot(data = arthritis.table, mapping = aes(x = week, y = perc.good, group = treatment, color = treatment)) +
  geom_line() +
  ylab(label = "% \"Good\"") +
  xlab(label = "Week") +
  scale_color_manual(name = "Treatment", values = c("#4444EC", "#EC4444"), labels = c("Placebo", "Auranofin"))
print(status.plot)
```
Ignoring potential effects of age and sex, the percent of subjects self-reporting a "good" status appears to diverge between the two treatments. In the placebo treatment, the percent of subjects with "good" status _decreases_ dramatically over weeks -1 through 4, then increases over weeks 4 through 12. In contrast, the Auranofin treatment showed _increases_ in the percent reporting "good" status over weeks -1 through 4, followed by a decrease between weeks 4 and 8, then a slight increase between weeks 8 and 12.

#### Is the study design balanced with respect to the timing of measurements? Briefly justify your answer. 

The study _design_ is balanced with respect to the timing of measurements, as there are the same number of measurements, made at the same time, across all subjects.

#### Are the data complete? Briefly justify your answer. 

The data are not complete; not all subjects have a complete set of all observations.

#### Characterize the pattern of change over time in the proportion with a “good” status by sex.

```{r arthritis-status-by-sex-over-time, fig.height = 3}
by.sex.table <- arthritis.long %>%
  group_by(treatment, week, sex) %>%
  summarize(count = sum(!is.na(status)),
            num.good = sum(status == 1, na.rm = TRUE))
by.sex.table$prop.good <- round((by.sex.table$num.good / by.sex.table$count), 3)

sex.names <- c(F = "Female", M = "Male")
status.by.sex.plot <- ggplot(data = by.sex.table, 
                             mapping = aes(x = week, 
                                           y = prop.good, 
                                           group = treatment, 
                                           color = treatment)) +
  geom_line() +
  facet_grid(. ~ sex, labeller = labeller(sex = sex.names)) +
  ylab(label = "Prop. \"Good\"") +
  xlab(label = "Week") +
  scale_color_manual(name = "Treatment", 
                     values = c("#4444EC", "#EC4444"), 
                     labels = c("Placebo", "Auranofin"))
print(status.by.sex.plot)
```

There are strikingly different patterns of change over time in the proportion of subjects with a "good" status between females and males. In the placebo treatment, there is a general decrease in the proportion of female subjects reporting "good" status, showing an increase only in the week 4 to week 8 period. The proportion of males reporting a "good" status in the placebo treatment shows an initial decrease (weeks -1 through 4), followed by an increase (weeks 4 through 12). In the Auranofin treatment, females had a sharp increase in the proportion of subjects with "good" status (weeks -1 through 4), followed by a decrease (weeks 4 through 12). Males in the Auranofin treatment showed a general increase in the proportion of subjects with "good" status, except for the week -1 to 0 period, where the proportion was constant and the week 4 to 8 period, where the proportion decreased. The pattern of change over time for males appears more similar to the trend when sex is ignored (previous plot), likely reflecting the fact that the study included more males than females.

<!--
In the following questions, use PROC GENMOD in SAS to fit marginal logistic regression models using the approach of generalized estimating equations (GEE) to obtain parameter estimates. The time variable, T, is defined to be the categorical variable with 4 levels: taking the value 99 for measurements at weeks -1 and 0 (i.e. for both “baseline” measurements prior to randomization) and values 4, 8 and 12 for the measurements at weeks 4, 8 and 12. The treatment variable, X, is an indicator variable taking the value 1 if a subject received auranofin (A) and the value 0 if placebo (P). The study investigator suggests fitting a marginal logistic regression model for the repeated arthritis self-assessment measurements as the outcome variable and with the T variable and a T*X interaction variable as categorical explanatory variables. He suggests using an independence working correlation matrix to describe the within-subject correlation structure (i.e. using the TYPE=IND on the REPEATEDstatement).
-->
```{r data-prep-1.2}
# Setup variables for GEE
# Using time instead of T, because T is reserved (TRUE)
arthritis.long$time <- arthritis.long$week
arthritis.long$time[arthritis.long$time %in% c(-1, 0)] <- 99
arthritis.long$time <- factor(arthritis.long$time, levels = c(99, 4, 8, 12))

# X is indicator of treatment (0 = P(lacebo), 1 = A(uranofin))
arthritis.long$X <- 0
arthritis.long$X[arthritis.long$treatment == "A"] <- 1
arthritis.long$X <- factor(arthritis.long$X, levels = c(0, 1))
```

### 2a. Define appropriate notation and write down the algebraic form for the model that will be fitted using the GEE method, including any assumptions.

**Assume**:

1. The mean response is conditional on covariates only (no random effects) and is assumed to be related to covariates through a known link function ($g(\cdot)$), which has a linear relationship with covariates:

$$
g(\mu_{ij}) = \beta_{1}X_{1ij} + \beta_{2}X_{2ij} + \beta_{3}X_{3ij} + ... + \beta_{p}X_{pij}
$$
where $\mu_{ij} = E(Y_{ij}|X_{ij})$.

2. The conditional variance is related to the mean by a known variance function ($v(\cdot)$) and scale parameter ($\phi$):

$$
Var(Y_{ij}|X_{ij}) = \phi v(\mu_{ij})
$$

3. The conditional association between responses of the same subject are a function of the mean an a set of association parameters, $\alpha$. 

Additional assumption: Data are missing completely at random (MCAR).

**Model definition**

1. Link function:
$$
\begin{aligned}
  logit(\mu_{ij}) = &\beta_1 + \beta_2 T_{i2} + \beta_3 T_{i3} + \beta_4 T_{i4} + \\
  &\beta_5 T_{i1} \times X_i + \beta_6 T_{i2} \times X_i + \beta_7 T_{i3} \times X_i + \beta_8 T_{i4} \times X_i
\end{aligned}
$$
Where:
$$
\begin{array}{l}
  T_{i1} = 
  \begin{cases}
      1, & \text{if measurement taken at weeks -1 or 0} \\
      0, & \text{otherwise}
  \end{cases} \\
  T_{i2} = 
  \begin{cases}
      1, & \text{if measurement taken at week 4} \\
      0, & \text{otherwise}
  \end{cases} \\
  T_{i3} = 
  \begin{cases}
      1, & \text{if measurement taken at week 8} \\
      0, & \text{otherwise}
  \end{cases} \\
  T_{i4} = 
  \begin{cases}
      1, & \text{if measurement taken at week 12} \\
      0, & \text{otherwise}
  \end{cases} \\
  X_i = 
  \begin{cases}
      1, & \text{Subject in Auranofin treatment} \\
      0, & \text{Subject in placebo treatment}
  \end{cases} \\
\end{array}
$$
Note $T_{i2}$ is included only in an interaction effect ($T_{i2} \times X_i$). This is because R will create a coefficient for every level of a factor in an interaction term. See more details in the answer to 3(a).

2. Variance

$$
Var(Y_{ij}|X_{ij}) = \mu_{ij}(1 - \mu_{ij}) \enspace \text{(Bernoulli variance, assuming no overdispersion, }\phi = 1 \text{)}
$$
3. Within-subject association has an "independent" structure for the working correlation, asserting no correlation between observations within a subject:

$$
Corr(Y_{ij}, Y_{ik}) = 
\begin{cases}
  1, & j = k \\
  0, & j \neq k
\end{cases}
$$

#### b. The model includes the variable T as a main effect. What is being assumed about subjects’ arthritis status at weeks -1 and 0? Briefly critique this assumption.

This assumption effectively ignores within-subject variation between weeks -1 and 0. While there may not be an _a priori_ reason to expect a change over this pre-randomization period, information about within-subject variation is unnecessarily discared. This _might_ lead to mis-specifying the correlation structure of within-subject associations. Misspecification of the covariance structure has the potential to reduce the power of the analysis.

#### c. The model does not include the variable X as a main effect. What assumption is therefore being made? Briefly describe why this assumption is reasonable in this study.

The assumption is that subjects assigned to placebo treatment have the same expected probability of reporting "good" arthritis status as subjects assigned to the Auranofin treatment _before_ the treatment is actually applied ($Pr(Y = "good"|X = 0, T = 99)) = Pr(Y = "good"|X = 1, T = 99)$). This is a reasonable assumption because it is a randomized trial.

#### d. In using the GEE method in the presence of missing data, the assumption of missingness completely at random (MCAR) is required. In 1-2 sentences, describe what this means about the missing assessments.

Missing completely at random means that the probability that an observation is missing is independent of the value of observations that are made ($Y^O$) and the value of observations that are missing ($Y^M$). This is in contrast to assumption of data missing at random (MAR), where the probability that an observation is missing is dependent on the values that _were_ observed ($Y^O$), but unrelated to the values that are actually missing ($Y^M$).

#### Are there any features of the study dataset that might lead you to question this?
```{r dropouts-over-time}
dropouts <- colSums(apply(X = arthritis.data[, 5:9], MARGIN = 2, FUN = is.na))
```
The increasing number of missing observations over time suggests that subjects _might_ have skipped observations based on whether or not they would have reported "good" status. If so, this would indicate data are NMAR (where the probability of missing data is dependent on the values that are missing, $Y^M$), which would be less than ideal.

| Week | # Missing       |
|:----:|:---------------:|
| -1   | `r dropouts[1]` |
| 0    | `r dropouts[2]` |
| 4    | `r dropouts[3]` |
| 8    | `r dropouts[4]` |
| 12   | `r dropouts[5]` |

<!--In all questions that follow, assume that MCAR is reasonable.-->

### 3a. Fit the model described in question 2 using PROC GENMOD. Provide the PROC GENMOD code used and the estimates of the parameters in the mean part of the model obtained as well as empirical standard errors for these estimates. 
<!--[HINT: Add “/ param=ref “ after your list of variables in your class statement to ensure that SAS doesn’t try to outsmart you and add main effects of X back into the model].-->

```{r marginal-model-arthritis, echo = TRUE}
# corstr = "independence" >> effectively *no* within-subject correlation
arthritis.gee <- suppressMessages(gee(status ~ time + time:X,
                     id = id,
                     data = arthritis.long,
                     family = binomial,
                     corstr = "independence"))
arthritis.gee.summary <- summary(arthritis.gee)
arthritis.gee.coef <- coef(arthritis.gee.summary)
coef.table <- data.frame(arthritis.gee.coef)
coef.table$p <- 2*pnorm(q = abs(arthritis.gee.coef[,5]), lower.tail = FALSE)
coef.table <- round(coef.table, 3)
```
Note: Using `formula = status ~ time + time*X` excludes the `time99:timeX1` term, but brings in the main effect of X into the model. By default, R uses reference level coding (and this is true apparently for `gee`, which has `contr.treatment` for both `time` and `X` in the `contrasts` element of the `gee` output), which is what the `PARAM = REF` command in the `CLASS` Statement of `PROC GENMOD` appears to be doing.

Parameter estimates and empirical ("robust") standard errors:

| Coefficient | Covariate  | Estimate                   | Standard Error                | $P(\beta = 0)$ |
|:-----------:|:-----------|:--------------------------:|:-----------------------------:| :------------------:|
| $\beta_1$   | Intercept  | `r coef.table$Estimate[1]` | `r coef.table$Robust.S.E.[1]` | `r coef.table$p[1]` |
| $\beta_2$   | Time = 4   | `r coef.table$Estimate[2]` | `r coef.table$Robust.S.E.[2]` | `r coef.table$p[2]` |
| $\beta_3$   | Time = 8   | `r coef.table$Estimate[3]` | `r coef.table$Robust.S.E.[3]` | `r coef.table$p[3]` |
| $\beta_4$   | Time = 12  | `r coef.table$Estimate[4]` | `r coef.table$Robust.S.E.[4]` | `r coef.table$p[4]` |
| $\beta_5$   | Time99 * A | `r coef.table$Estimate[5]` | `r coef.table$Robust.S.E.[5]` | `r coef.table$p[5]` |
| $\beta_6$   | Time4 * A  | `r coef.table$Estimate[6]` | `r coef.table$Robust.S.E.[6]` | `r coef.table$p[6]` |
| $\beta_7$   | Time8 * A  | `r coef.table$Estimate[7]` | `r coef.table$Robust.S.E.[7]` | `r coef.table$p[7]` |
| $\beta_8$   | Time12 * A | `r coef.table$Estimate[8]` | `r coef.table$Robust.S.E.[8]` | `r coef.table$p[8]` |
For coefficients $\beta_5$ - $\beta_8$, the covariate is the interaction between time and treatment with Auranofin.

#### b. How is this model different from an ordinary logistic regression analysis that assumes that all observations are independent? Note here the question is not “how do the results from the two analyses differ for these data”, but how the models themselves differ – that is, why would they give different results. 

In general, ordinary and marginal models differ primarily in how they deal with within-subject correlations. In the ordinary logistic regression model, all observations, even those made on the same subject at different times, are assumed to be independent. In contrast, the marginal logistic model, within-subject correlations are accounted for in the "working" correlation matrix. Failing to account for within-subject non-independence may lead to misleading inferences, as the standard errors on parameter estimates will be smaller, potentially leading to false positives (rejecting $\beta = 0$ even when true). However, in this particular case, because we set `corstr = "independence"` in our marginal model, there is effectively _no_ within-subject correlation in the working correlation matrix (See also documentation on [SAS GENMOD](https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_genmod_sect043.htm)).

#### Which approach do you prefer and why?

In general, I would prefer the logistic marginal model over the ordinary logistic regression analysis because marginal models relax the assumption of independence among all samples, which is most likely violated in a longitudinal study such as this one. Marginal models also do not require the assumption of homogeneity of variance through time. The relaxation of these assumptions affords more confidence in inferences, as standard errors for parameter estimates are not artificially small as they would be for an ordinary logistic regression model on these longitudinal data.

### 4a. Include in your code a Wald test of the joint hypothesis that there is no effect of auranofin compared with placebo at all three of weeks 4, 8 and 12. Run this PROC GENMOD analysis and provide the PROC GENMOD code used, as well as the estimates of the parameters in the mean part of the model obtained, the standard errors for these estimates, p-values for tests that each parameter equals zero, and the p-value from the Wald test of joint hypothesis.
<!--[HINT: Use the TYPE3 and WALD options together on the MODEL statement]-->
<!-- For aods3::wald.test in R, specify the Terms to run the test for, not the L matrix-->

Testing $\beta_{6} = \beta_{7} = \beta_{8} = 0$ via $W^2 = (L\hat{\beta})'\{L\widehat{Cov}(\hat{\beta})L'\}^{-1}(L\hat{\beta})$, where:

$$
L\hat{\beta} = 
\begin{pmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
  \hat{\beta_{6}} \\
  \hat{\beta_{7}} \\
  \hat{\beta_{8}}
\end{pmatrix}
$$

For this Wald test, I relied on the previous marginal model estimates and variance-covariance matrix. For parameter estimates, standard errors, and p-values of the $\beta=0$ for individual parameters, see table in the answer to 3(a), above.
```{r joint-wald-test, echo = TRUE}
# For the call to aods3::wald.test:
#   b     : The vector of parameter estimates (betas)
#   varb  : The variance-covariance matrix
#   Terms : Index of one ore more coefficients to test
arthritis.joint.wald <- wald.test(b = arthritis.gee.coef[, 1],
                                  varb = arthritis.gee$robust.variance,
                                  Terms = c(6:8))
```
For the joint hypothesis, $W^2 = `r round(arthritis.joint.wald$result$chi2["chi2"], 3)`$ and $p = `r round(arthritis.joint.wald$result$chi2["P"], 3)`$. We thus reject the null hypothesis of $\beta_{6} = \beta_{7} = \beta_{8} = 0$.

#### b. What do you conclude concerning the effect of auranofin versus placebo?
<!--[Note: do not just provide a p-value from the test of the joint hypothesis; instead provide a broader interpretation of the results concerning any treatment differences].-->

The proclivity for the average Auranofin subject's arthritis status to change over time is significantly different from the average subject in the placebo treatment. The joint Wald test of a post-baseline effect of Auranofin on arthritis status was significant ($W^2 = `r round(arthritis.joint.wald$result$chi2["chi2"], 3)`$, $p = `r round(arthritis.joint.wald$result$chi2["P"], 3)`$). Additionally, in the test of individual time points, the effect of Auranofin treatment on the change over time at the week 4 measurement was also significant (`r coef.table$p[6]`), while effects of Auranofin treatment on other weeks were not. The estimate for this interaction effect between Auranofin and time in week 4 is `r coef.table$Estimate[6]`. Inference based on this parameter requires comparing the week 4 response for an average subject in the placebo group:

$$
logit(\mu_{ij}) = \beta_1 + \beta_2 T_{i2}
$$

with an average subject in the Auranofin treatment:
$$
logit(\mu_{ij}) = \beta_1 + \beta_2 T_{i2} + \beta_6 T_{i2} \times X_i
$$

The odds ratio between average subjects in placebo and treatment groups is then e^$\beta_{6}$ = e^`r coef.table$Estimate[6]`^ = `r round(exp(coef.table$Estimate[6]), 3)`. That is, treatment with Auranofin causes a `r round(exp(coef.table$Estimate[6]), 3)`-fold increase in the odds of an average subject to have a "good" status in week 4 compared with the average subject receiving the placebo treatment.

#### c. The default test of the joint hypothesis within SAS (obtained by using the TYPE3 option without the WALD option) is not a Wald test. Review the SAS documentation and identify what type of test it is and why SAS uses it as the default rather than the Wald test.

The default test is a likelihood ratio test ("score test" per SAS documentation). LRTs are likely used as default because Wald tests may be too liberal with smaller samples. See: [https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_genmod_sect035.htm](https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_genmod_sect035.htm).

### 5. Another investigator questions whether it is appropriate to use an unstructured log odds ratio correlation structure. Now using PROC GENMOD, fit the same model but using an unstructured log odds ratio structure for the working association structure. Write out how your model formulation given in 2(a) changes under this alternative assumption. Describe whether the conclusions of your GEE analyses are sensitive to the change in association structure.

The investigator is interested in modeling within-subject correlation with unstructured log-odds ratio pattern, rather than a working correlation matrix. So point 3, which was:

$$
Corr(Y_{ij}, Y_{ik}) = 
\begin{cases}
  1, & j = k \\
  0, & j \neq k
\end{cases}
$$
We instead model within-subject correlation via:

$$
\text{log} OR(Y_{ij}, Y_{ik}|X_{ij}, X_{ik}) = \alpha_{jk}
$$
where

$$
OR(Y_{ij}, Y_{ik}) = \frac{Pr(Y_{ij} = 1, Y_{ik} = 1) \times Pr(Y_{ij} = 0, Y_{ik} = 0)}{Pr(Y_{ij} = 1, Y_{ik} = 0) \times Pr(Y_{ij} = 0, Y_{ik} = 1)}
$$
This releases the constraints imposed by binary response variables. To test this, I tried using the `alr` function in the `alr` package:

```{r log-odds-evaluated, include = FALSE}
# Produces S.E. = 0 and Z = Inf for all coefficients
arthritis.gee.logOR <- suppressMessages(alr(status ~ time + time:X,
                           id = id,
                           data = arthritis.long,
                           depm = "exchangeable",
                           ainit = 0.01))
arthritis.gee.logOR.coef <- coef(summary(arthritis.gee.logOR))
```

```{r log-odds-echoed, eval = FALSE, echo = TRUE}
# Produces S.E. = 0 and Z = Inf for all coefficients
arthritis.gee.logOR <- suppressMessages(alr(status ~ time + time:X,
                           id = id,
                           data = arthritis.long,
                           depm = "exchangeable",
                           ainit = 0.01))
arthritis.gee.logOR.coef <- coef(summary(arthritis.gee.logOR))
```

However, I was unable to assess the impact of this association structure on the model because the `alr` function returned uninformative standard errors and z scores (0 and -Inf/Inf, respectively). While the parameter estimates between the two within-subject correlation models were identical (as expected), I suspect the standard errors were not. Accounting for within-subject correlation with an unstructured log-odds ratio correlation structure likely would increase the standard errors for parameter estimates, although perhaps not so much as to qualitatively change the inference.

### 6. The investigator is interested in knowing whether the results comparing the effect of auranofin versus placebo on arthritis status over time is affected by any imbalance in sex between the treatment groups. Conduct analysis to address this specific issue. Briefly describe how you approached this (include your PROC GENMOD code) and what your key findings are.

To test for an effect of sex, I expanded the model from 2(a) to include covariates for sex and sex * time interactions:
$$
\begin{aligned}
  logit(\mu_{ij}) = &\beta_1 + \beta_2 Sex_{i} + \beta_3 T_{i2} + \beta_4 T_{i3} + \beta_5 T_{i4} + \\
  &\beta_6 T_{i1} \times X_i + \beta_7 T_{i2} \times X_i + \beta_8 T_{i3} \times X_i + \beta_9 T_{i4} \times X_i + \\
  &\beta_{10} T_{i2} \times Sex_i  + \beta_{11} T_{i3} \times Sex_i  + \beta_{12} T_{i4} \times Sex_i
\end{aligned}
$$

```{r marginal-compare-sexes-evaluated, include = FALSE}
# Run expanded model
arthritis.s.gee <- suppressMessages(gee(status ~ sex + time + time:X + time:sex,
                     id = id,
                     data = arthritis.long,
                     family = binomial,
                     corstr = "independence"))
arthritis.s.gee.coef <- coef(summary(arthritis.s.gee))
```

```{r marginal-compare-sexes-echoed, echo = TRUE, eval = FALSE}
# Run expanded model
arthritis.s.gee <- suppressMessages(gee(status ~ sex + time + time:X + time:sex,
                     id = id,
                     data = arthritis.long,
                     family = binomial,
                     corstr = "independence"))
```

I then ran a multivariate Wald test on the hypothesis that there is no significant sex * time interaction, $H_0: \beta_{10} = \beta_{11} = \beta_{12} = 0$.
```{r}
# Run multivariate Wald test
arthritis.s.joint.wald <- wald.test(b = arthritis.s.gee.coef[, 1],
                                  varb = arthritis.s.gee$robust.variance,
                                  Terms = c(10:12))
```

For the joint hypothesis, $W^2 = `r round(arthritis.s.joint.wald$result$chi2["chi2"], 3)`$ and $p = `r round(arthritis.s.joint.wald$result$chi2["P"], 3)`$. We thus do not reject the null hypothesis of $\beta_{10} = \beta_{11} = \beta_{12} = 0$, and conclude the results are not likely affected by any imbalance in sex between the treatment groups.

### 7. Summarize the objective, methods, results and conclusions of the study in an informative structured abstract suitable for a medical journal or conference (i.e. with sections under the headings: Objective, Methods, Results, Conclusions). Include key numerical results. The abstract should be no longer than 500 characters including spaces and cannot include tables or figures.

#### Objective
The objective of this work was to test the effects of auranofin on arthritis.

#### Methods
Subjects were randomized into auranofin and placebo treatments, then asked whether or not they would categorize their arthritis status as "good". Status was assessed one week prior to start of treatment, and at 0, 4, 8, and 12 weeks after the start of treatment. We used marginal mixed models to assess the effect of treatment and Wald tests to assess significance.

#### Results
In a joint Wald test, the average subject receiving auranofin was more likely to have "good" arthritis status than a subject receiving a placebo as measured in weeks 4, 8, and 12 ($W^2 = `r round(arthritis.joint.wald$result$chi2["chi2"], 3)`$, $p = `r round(arthritis.joint.wald$result$chi2["P"], 3)`$).

#### Conclusions
The results presented here demonstrate that treatment with auranofin provides a significant improvement in arthritis status over placebo treatments.

_Note: there was no way for me to provide an informative abstract with only 500 **characters**._

## Question 2
### The investigator at Center=1 is interested in characterizing the distribution of risk among subjects at her center. _In the following, only include the subset of subjects with Center=1 in the analysis._
<!--
This question is based on the study that was featured in Homework 8. Briefly, recall that the Skin Cancer Prevention Study was a randomized, double-blind, placebo-controlled multi-center clinical trial designed to test the effectiveness of beta- carotene in the prevention of non-melanoma skin cancer in high-risk subjects. Subjects were randomized to either placebo or 50 mg of beta-carotene per day for 5 years. Subjects were examined once per year and the outcome variable, Y, is a count of the number of new skin cancers per year. 

Selected data from the study are in the dataset called “skin.txt” on the course web site. Each row of the dataset contains the following 9 variables: ID, Center, Age, Skin, Gender, Exposure, Y, Treatment, Year.
These variables take values as follows:
ID: Subject identifier number
Center: Identifier number for center of enrollment.
Age: Subject’s age in years at randomization
Skin: Skin type (1=burns; 0 otherwise) [evaluated at randomization and doesn’t
change with time]
Gender: 1=male; 0=female
Exposure: Count of number of previous skin cancers [prior to randomization]
Y: Count of number of new skin cancers in the Year of follow-up
Treatment: 1=beta-carotene; 0=placebo
Year: Year of follow-up after starting randomized treatment

As in HW8, you may assume that the counts of new skin cancers, Y, are from exact one-year periods (so that no offset term is needed). 
-->

```{r read-skin-data}
# Read data
skin.data <- read.delim(file = "data/skin.txt", sep = " ", header = FALSE)

# Trim first & last columns (created by whitespace)
skin.data <- skin.data[, -c(1, ncol(skin.data))]

# Identify columns
colnames(skin.data) <- c("id", "center", "age", "skin", "gender", "prior.count", "new.count", "treatment", "year")

# Drop data from center != 1
skin.data <- skin.data[skin.data$center == 1, ]

# Set factors as needed
factor.cols <- c("id", "center", "skin", "gender", "treatment")
for (c in factor.cols) {
  skin.data[, c] <- factor(skin.data[, c])
}
```

#### a) Provide an algebraic definition for a generalized linear marginal model in which the only effects are for the intercept and Year (as a continuous variable). Fit this model and provide a table from your SAS output which includes the estimates of the parameters in your model.

1. Mean response:
$$
log(\mu_{ij}) = \beta_1 + \beta_2 \times Year_{ij}
$$

2. Variance:
$$
Var(Y_{ij}|X_{ij}) = \phi(\mu_{ij})
$$

3. Within-individual correlation structure:
$$
Corr(Y_{ij}, Y_{ik}) = \alpha_{jk}
$$
(compound symmetric, or exchangable; autoregressive might also be appropriate, though)

```{r skin-gee, include = FALSE, echo = TRUE}
skin.gee <- gee(new.count ~ year,
                id = id,
                data = skin.data,
                family = poisson,
                corstr = "exchangeable")
```

```{r skin-gee-results}
skin.gee.summary <- summary(skin.gee)
skin.gee.coef <- data.frame(coef(skin.gee.summary))
skin.gee.coef$p <- 2*pnorm(q = abs(skin.gee.coef[, 5]), lower.tail = FALSE)
skin.gee.table <- round(skin.gee.coef, 3)
skin.gee.table$p <- as.character(skin.gee.table$p)
skin.gee.table$p[skin.gee.table$p < 0.001] <- "< 0.001"
```

| Coefficient | Covariate | Estimate | S.E.        | Z | Pr($\beta$ = 0) |
|:------------|:---------:|:--------:|:-----------:|:-:|:---------------:|
| $\beta_1$   | Intercept | `r skin.gee.table$Estimate[1]` | `r skin.gee.table$Robust.S.E.[1]` | `r skin.gee.table$Robust.z[1]` | `r skin.gee.table$p[1]` |
| $\beta_2$   | Year      | `r skin.gee.table$Estimate[2]` | `r skin.gee.table$Robust.S.E.[2]` | `r skin.gee.table$Robust.z[2]` | `r skin.gee.table$p[2]` |

#### b) Provide an algebraic definition for a generalized linear mixed model (GLMM) in which the only fixed effects are for the intercept and Year (as a continuous variable), and the only random effect is the intercept.

Response for the mean:
$$
log(E(Y_{ij}|X_{ij}, b_i)) = \beta_1 + \beta_2 Year_{ij} + b_{1i}
$$
Variance:
$$
Var(Y_{ij}|b_i) = E(Y_{ij}|X_{ij}, b_i) \enspace \text{(Poisson variance)}
$$
where $b_i \sim N(0, \sigma^2_b)$. And given the within-subject random effects, $b_i$, observations $Y_{i1}, Y_{i2}, ... Y_{in_i}$ for the _i^th^_ subject are mutually independent (conditional independence).

#### What is being assumed about how the distribution of risk among subjects changes with time?
This random intercepts model assumes that there are no random slope effects. That is, subjects do not exhibit unique changes in the number of skin cancers over time.

#### c) Fit your chosen GLMM in SAS and provide the SAS code for the fitting the model (just provide the code from the PROC that you use). Provide a table from your SAS output which includes the estimates for the parameters in your GLMM, and provide careful interpretation of the Year term.

```{r skin-glmm, echo = TRUE}
# Use nAGQ > 1 for better accuracy
skin.glmm <- glmer(new.count ~ year + (1 | id),
                    data = skin.data,
                    nAGQ = 1, # 1 point evaluated = Laplace approximation
                    family = poisson(link = "log"))
```

```{r skin-glmm-results}
skin.glmm.summary <- summary(skin.glmm)
skin.glmm.coef <- coef(skin.glmm.summary)
skin.glmm.table <- data.frame(round(skin.glmm.coef, 3))
skin.glmm.table$p.text <- as.character(skin.glmm.table$Pr...z..)
skin.glmm.table$p.text[skin.glmm.table$Pr...z.. < 0.001] <- "< 0.001"
```

| Coefficient | Covariate | Estimate | S.E.        | Z | Pr($\beta$ = 0) |
|:------------|:---------:|:--------:|:-----------:|:-:|:---------------:|
| $\beta_1$   | Intercept | `r skin.glmm.table$Estimate[1]` | `r skin.glmm.table$Std..Error[1]` | `r skin.glmm.table$z.value[1]` | `r skin.glmm.table$p[1]` |
| $\beta_2$   | Year      | `r skin.glmm.table$Estimate[2]` | `r skin.glmm.table$Std..Error[2]` | `r skin.glmm.table$z.value[2]` | `r skin.glmm.table$p[2]` |

For each unit of time (i.e. each year), the number of new skin cancers for an individual subject is expected to change by a factor of $e^{\beta_{2}} = e^{`r skin.glmm.table$Estimate[2]`} = `r round(exp(skin.glmm.coef[2, 1]), 3)`$.

#### d) Are the estimates for the fixed intercept terms the same or different in the GLMM compared with the marginal model fitted in question 2a? Why are they the same or different?

The GEE model estimated $\beta_1 = `r skin.gee.table$Estimate[1]`$, while GLMM model estimated $\beta_1 = `r skin.glmm.table$Estimate[1]`$. The GEE (marginal) model is describing the population average intercept (or number of new cancers at year 0), while the GLMM model described the average subject-specific intercept. Were we modeling a linear response, e.g. $E(Y_{ij})$ instead of $log(E(Y_{ij})$, the estimates should be the same.

#### e) Use the parameter estimates from your GLMM and your model definition to characterize the distribution of expected counts of new skin cancers among subjects at center 1 during their first year of follow-up.

```{r random-intercept-variance}
sigma.b <- skin.glmm.summary$varcor[[1]][1]
```

We can characterize the distribution of expected skin counts of new cancers in the first year with the model:
$$
log(E(Y_{ij}|X_{ij}, b_i)) = \beta_1 + \beta_2 Year_{ij} + b_{1i}
$$

Substituting in values for the $\beta$ estimates:
$$
log(E(Y_{ij}|X_{ij}, b_i)) = `r skin.glmm.table$Estimate[1]` `r skin.glmm.table$Estimate[2]` \times Year_{ij} + b_{1i}
$$

Setting $Year_{ij} = 1$, this becomes:
$$
log(E(Y_{ij}|X_{ij}, b_i)) = `r skin.glmm.table$Estimate[1] + skin.glmm.table$Estimate[2]` + b_{1i}
$$

The remaining step is to generate the distribution of expected new counts from the estimated variance in $b_{1i}$. From the `varcor` element of the `summary.glmer` object, the standard deviation of $b_i = `r round(sigma.b, 3)`$, so $b_i \sim N(0, `r round(sigma.b^2, 3)`)$. Distributionally, we can say:
$$
log(E(Y_{ij}|X_{ij}, b_i)) \sim N(\beta_1 + \beta_2, \sigma^2_b)
$$
or
$$
log(E(Y_{ij}|X_{ij}, b_i)) \sim N(`r skin.glmm.table$Estimate[1] + skin.glmm.table$Estimate[2]`, `r round(sigma.b^2, 3)`)
$$

This can be seen graphically:
```{r new-counts-distribution, echo = TRUE, fig.height = 2.5}
# Generate values based on normal distribution with mean 0 and s.d. = s.d.(bi)
intercepts <- rnorm(n = 1000, mean = 0, sd = sigma.b)
# Now create vector based on beta estimates
expected <- data.frame(log.counts = skin.glmm.table$Estimate[1] + skin.glmm.table$Estimate[2] + intercepts, new.counts = NA)
# Use link function to get new counts
expected$new.counts <- exp(expected$log.counts)
# Plot the distribution of expected number of new cancers
hist.plot <- ggplot(data = expected, mapping = aes(x = new.counts)) +
  geom_histogram() +
  ylab(label = "Frequency") +
  xlab(label = "New cancers") +
  ggtitle("Expected distribution for year 1 at center 1")
print(hist.plot)
```

## Optional Extra Credit Problem
<!--
(This problem is meant to offer another chance to demonstrate understanding of some of the material on the mid-term. If you choose to do this problem and your score is higher than your mid-term grade, then your mid-term grade will be reweighted to be New Midterm Grade = .7*Old Midterm Grade + .3*Extra Credit Problem).

Onychomycosis, popularly known as toenail fungus, is a fairly common condition that not only can disfigure and sometimes destroy the nail but that also can lead to social and self-image issues for sufferers. Tight-fitting shoes or hosiery, the sharing of common facilities such as showers and locker rooms, and toenail polish are all thought to be implicated in the development of onychomycosis. This question relates to data from a study conducted by researchers that recruited sufferers of a particular type of onychomycosis, dermatophyte onychomycosis. The study conducted by the researchers was focused on comparison of two oral medications, terbinafine (given as 250 mg/day, denoted as treatment 1 below) and itraconazole (given as 200 mg/day, denoted as treatment 2 below). The trial was conducted as follows. 200 sufferers of advanced toenail dermatophyte onychomycosis in the big toe were recruited, and each saw a physician, who removed the afflicted nail. Each subject was then randomly assigned to treatment with either terbinafine (treatment 1) or itraconazole (treatment 2). Immediately prior to beginning treatment, the length of the unafflicted part of the toenail (which was hence not removed) was recorded (in millimeters). Then at 1 month, 2 months, 3 months, 6 months, and 12 months, each subject returned, and the length of the unafflicted part of the nail was measured again. A longer unafflicted nail length is a better outcome. Also recorded on each subject was gender and an indicator of the frequency with which the subject visited a gym or health club (and hence might use shared locker rooms and/or showers).

The data are available in the file toenail.txt on the class web page. The data are presented in the form of one data record per observation; the columns of the data set are as follows:
1 Subject id
2 Health club frequency indicator (= 0 if once a week or less, = 1 if more than once a
week)
3 Gender indicator (= 0 if female, = 1 if male)
4 Month
5 Unafflicted nail length (the response, mm)
6 Treatment indicator (= 1 if terbinafine, = 2 if itraconazole)
-->

```{r read-toenail-data}
# Read the data in
toenail.data <- read.delim(file = "data/toenail.txt", header = FALSE, sep = "")

# Assign column names
colnames(toenail.data) <- c("id", "freq", "gender", "month", "length", "treatment")

# Create factor columns
toenail.data$treat.cat <- 0
toenail.data$treat.cat[toenail.data$treatment == 2] <- 1
toenail.data$treat.cat <- factor(toenail.data$treat.cat, levels = c(0, 1))
```

_In answering these scientific questions of interest, clearly write out the analytic models you consider for answering these questions. Clearly outline your decision making process for how you selected your final models. Fit your chosen final models and report to the project investigators on the stated scientific questions of interest._

### The researchers had several questions, which they stated to you as follows:
### 1. Is there a difference in the pattern of change of lengths of the unafflicted part of the nail between subjects receiving terbinafine and itraconazole over a 12 month period? Does one treatment show results more quickly?

For this question, I chose to first compare two linear mixed models, a random intercepts model and a random intercepts and slopes model, selecting the best model based on a likelihood ratio test.

In both models, the fixed effects of time and treatment are the same:
$$
\begin{array}{l}
  Month_{ij} = \text{Month of measurement} \\
  Trt_i = 
  \begin{cases}
    1, & \text{if subject received itraconazole treatment} \\
    0, & \text{if subject received terbinafine treatment}
  \end{cases}
\end{array}
$$
(Note the main effect of treatment is not included in the model, as subjects were randomized before treatment, thus we can appropriately assume the mean length at month 0 is the same in the two treatment groups)

And within-subject error:
$$
\begin{array}{l}
  \varepsilon_{ij} \sim N(0, R_{i}) \\
  R_{i} = \sigma^2I \enspace \text{(}I\text{ is an }n_i \times n_i \text{identity matrix)}
\end{array}
$$
The assumptions around $\varepsilon_{ij}$ assert that the errors _within_ a subject are uncorrelated and homogeneously distributed (i.e. conditional independence).

The random intercepts and slopes model, testing the effect of terbinafine and itraconazole on nail growth, treating time as a continuous predictor:
$$
E(Y_{ij}) = \beta_{1} + \beta_{2}Month_{ij} + \beta_{3}Month_{ij} \times Trt_{i} + b_{1i} + b_{2i} \times Month_{ij}
$$
where $b_{1i}$ and $b_{2i}$ model the random intercepts and slopes, respectively, of between-subject variation.

Assumptions on between-subject variation for random intercepts and slopes model:
$$
\begin{array}{l}
  b_{i} \sim MVN(0, G) \\
  G = 
    \begin{pmatrix}
      g_{11} & g_{12} \\
      g_{21} & g_{22}
    \end{pmatrix} =
    \begin{pmatrix}
      Var(b_{1i}) & Cov(b_{1i}, b_{2i}) \\
      Cov(b_{1i}, b_{2i}) & Var(b_{2i})
    \end{pmatrix}
\end{array}
$$
The random intercepts model, testing the effect of terbinafine and itraconazole on nail growth, treating time as a continuous predictor:
$$
E(Y_{ij}) = \beta_{1} + \beta_{2}Month_{ij} + \beta_{3}Month_{ij} \times Trt_{i} + b_{1i}
$$
This assumes $Var(b_{2i}) = 0$ and $Cov(b_{1i}, b_{2i}) = 0$, i.e. there is no random effect on slope.

Selecting between these mdoels, I run a linear mixed-effect model and compare the likelihoods:

```{r toenail-lmm-lrt, echo = TRUE}
# Random intercepts model
toenail.lmm.1random <- lme(fixed = length ~ month + month * treat.cat,
                           random = ~1 | id,
                           data = toenail.data)

# Random intercepts & slopes model
toenail.lmm.2random <- lme(fixed = length ~ month + month * treat.cat,
                           random = ~1 + month| id,
                           data = toenail.data)
```

```{r toenail-lmm-lrt-result}
# Calculate statistic for LRT
lrt.stat <- -2*(toenail.lmm.1random$logLik - toenail.lmm.2random$logLik)
lrt.p <- pchisq(q = lrt.stat, df = 2, lower.tail = FALSE)
lrt.p.text <- as.character(round(lrt.p, 3))
if (lrt.p < 0.001) {
  lrt.p.text <- "< 0.001"
}
```

The likeklihood ratio statistic is `r round(lrt.stat, 3)`. Comparing this with a $\chi^2$ distribution with 2 degrees of freedom, the likelihood ratio statistic is significant ($p `r lrt.p.text`$). Therefore, including the random slopes in the model provides a significantly better fit than the model with random intercepts alone. For all subsequent tests, the models will include random intercepts and slopes.

To test the investigator's question, recall the linear mixed model with random intercepts and slopes is:

$$
E(Y_{ij}) = \beta_{1} + \beta_{2}Month_{ij} + \beta_{3}Month_{ij} \times Trt_{i} + b_{1i} + b_{2i} \times Month_{ij}
$$
and the **hypothesis:** $H_{0}: \beta_{3} = 0$.

```{r toenail-lmm-treatment}
toenail.lmm.2random.coef <- coef(summary(toenail.lmm.2random))
toenail.lmm.2random.table <- data.frame(round(toenail.lmm.2random.coef, 3))
toenail.lmm.2random.table$p.text <- paste0("= ", toenail.lmm.2random.table$p.value)
toenail.lmm.2random.table$p.text[toenail.lmm.2random.table$p.value < 0.001] <- "< 0.001"
```

The REML estimate of the $\beta_3$ coefficient is `r toenail.lmm.2random.table$Value[4]`. This value is significantly different from 0 ($p `r toenail.lmm.2random.table$p.text[4]`$), so we reject the null hypothesis, $H_{0}: \beta_{3} = 0$ of no effect of treatment on rate of nail growth. These results indicate that on average, subjects receiving the itraconazole treatment experienced an increased rate of growth relative to subjects receiving the terbinafine treatment. **On average, subjects in the itraconazole treatment experienced nail growth of `r toenail.lmm.2random.table$Value[4]` mm per month _more_ than subjects in the terbinafine treatment.**

### 2. Is there an association between the pattern of change of nail lengths and gender and/or health club frequency in subjects taking terbinafine? This might indicate that this drug brings about relief more swiftly in some kinds of subject versus others.

```{r prepare-terbin-data}
terbin.data <- toenail.data[toenail.data$treat.cat == 0, ]
terbin.size <- length(unique(terbin.data$id))
```

The question focuses only on those subjects in the terbinafine treatment, so the data are restricted to only those subjects who received terbinafine (N = `r terbin.size`). The first thing is to determine whether to include a random slopes term in the model. The linear mixed effects models are:

Random intercepts:
$$
\begin{aligned}
E(Y_{ij}) = &\beta_{1} + \beta_{2} Month_{ij} + \beta_{3} Gender_{i} + \beta_{4} Freq_{i} + \\
&\beta_5 Gender_{i} \times Month_{ij} + \beta_6 Freq_{i} \times Month_{ij} + b_{1i}
\end{aligned}
$$

Random intercepts and slopes:
$$
\begin{aligned}
E(Y_{ij}) = &\beta_{1} + \beta_{2} Month_{ij} + \beta_{3} Gender_{i} + \beta_{4} Freq_{i} + \\
&\beta_5 Gender_{i} \times Month_{ij} + \beta_6 Freq_{i} \times Month_{ij} + b_{1i} + b_{2i} \times Month_{ij}
\end{aligned}
$$

Where:
$$
\begin{array}{l}
  Month_{ij} = \text{Month of measurement} \\
  Gender_i = 
  \begin{cases}
    0, & \text{female} \\
    1, & \text{male}
  \end{cases} \\
  Freq_i = 
  \begin{cases}
    0, & \text{visits health club once a week or less} \\
    1, & \text{visits health club more than once a week}
  \end{cases} \\
  b_{1i} = \text{random effect of intercept for subject }i \\
  b_{2i} = \text{random effect of slope for subject }i
\end{array}
$$
Note in the random intercepts model, $b_{2i} = 0$.

Comparing these two models with a likelihood ratio test:

```{r terbin-lmm-lrt, echo = TRUE}
# Random intercepts model
terbin.lmm.1random <- lme(fixed = length ~ month + gender + freq + month * gender + month * freq,
                           random = ~1 | id,
                           data = terbin.data)

# Random intercepts & slopes model
terbin.lmm.2random <- lme(fixed = length ~ month + gender + freq + month * gender + month * freq,
                           random = ~1 + month| id,
                           data = terbin.data)
```

```{r terbin-lmm-lrt-results}
# Calculate statistic for LRT
lrt.terbin.stat <- -2*(terbin.lmm.1random$logLik - terbin.lmm.2random$logLik)
lrt.terbin.stat.text <- as.character(round(lrt.terbin.stat, 3))
lrt.terbin.p <- pchisq(q = lrt.terbin.stat, df = 2, lower.tail = FALSE)
lrt.terbin.p.text <- as.character(round(lrt.terbin.p, 3))
if (lrt.terbin.p < 0.001) {
  lrt.terbin.p.text <- "< 0.001"
}
```

As previously, the random intercepts and slopes model provided a significantly better fit than did the random intercepts model ($2\Delta lnL = `r lrt.terbin.stat.text`$, $p `r lrt.terbin.p.text`$), so subsequent tests will include random intercepts and slopes.

The linear mixed effects model testing the effect of gender and frequency of health club visits on the rate of growth for subjects receiving terbinafine:
$$
\begin{aligned}
E(Y_{ij}) = &\beta_{1} + \beta_{2} Month_{ij} + \beta_{3} Gender_{i} + \beta_{4} Freq_{i} + \\
&\beta_5 Gender_{i} \times Month_{ij} + \beta_6 Freq_{i} \times Month_{ij} + b_{1i} + b_{2i} \times Month_{ij}
\end{aligned}
$$

There are two hypotheses we are testing:

+ No effect of gender on the rate of growth, $H_0: \beta_{5} = 0$
+ No effect of frequency of health club visits on the rate of growth, $H_0: \beta_{6} = 0$

```{r terbin-lmm-treatment}
terbin.lmm.2random.coef <- coef(summary(terbin.lmm.2random))
terbin.lmm.2random.table <- data.frame(round(terbin.lmm.2random.coef, 3))
terbin.lmm.2random.table$p.text <- terbin.lmm.2random.table$p.value
terbin.lmm.2random.table$p.text[terbin.lmm.2random.table$p.value < 0.001] <- "< 0.001"
```

Parameter estimates from linear mixed model:

| Coefficient | Covariate         | Value | t      | $Pr(\beta = 0)$ |
|:-----------:|:------------------|:-----:|:------:|:---------------:|
| $\beta_1$   | Intercept         | `r terbin.lmm.2random.table$Value[1]` | `r terbin.lmm.2random.table$t.value[1]` | `r terbin.lmm.2random.table$p.text[1]` |
| $\beta_2$   | Month             | `r terbin.lmm.2random.table$Value[2]` | `r terbin.lmm.2random.table$t.value[2]` | `r terbin.lmm.2random.table$p.text[2]` |
| $\beta_3$   | Gender (male)     | `r terbin.lmm.2random.table$Value[3]` | `r terbin.lmm.2random.table$t.value[3]` | `r terbin.lmm.2random.table$p.text[3]` |
| $\beta_4$   | Frequency (> once a week)        | `r terbin.lmm.2random.table$Value[4]` | `r terbin.lmm.2random.table$t.value[4]` | `r terbin.lmm.2random.table$p.text[4]` |
| $\beta_5$   | Month * Gender    | `r terbin.lmm.2random.table$Value[5]` | `r terbin.lmm.2random.table$t.value[5]` | `r terbin.lmm.2random.table$p.text[5]` |
| $\beta_6$   | Month * Frequency | `r terbin.lmm.2random.table$Value[6]` | `r terbin.lmm.2random.table$t.value[6]` | `r terbin.lmm.2random.table$p.text[6]` |

**There is no support for either an effect of gender ($p = `r terbin.lmm.2random.table$p.text[5]`$) nor an effect of the frequency of health club visits ($p = `r terbin.lmm.2random.table$p.text[6]`$) on the rate of nail growth for subjects in the terbinafine treatment.**

***
The R code used in this assignment can be found in a corresponding R Markdown document at [https://github.com/jcoliver/bios-684/blob/master/final-exam-oliver.Rmd](https://github.com/jcoliver/bios-684/blob/master/final-exam-oliver.Rmd).
